
# Where's Waldo? â€“ Automated Visual Search using YOLOv11

![Find Waldo Illustration](/assets/images/1.jpg "Find Waldo Illustration")

## ðŸ“œ Overview

This project implements an automated visual search system to detect Waldo in dense illustrations using the YOLOv11 object detection model. Each illustration is split into 640Ã—640 sub-images, with bounding box annotations for Waldo when present. The pipeline is optimized for small-object detection and rare-class scenarios, using tailored hyperparameters and targeted data augmentation.

---

## ðŸ“‚ Project Structure

```bash
where-is-waldo/
â”œâ”€â”€ assets/ # assets for readme
â”‚ â””â”€â”€ images/ # images for README.md
â”‚
â”œâ”€â”€ datasets/ # train and val datasets (populated from preprocess.py)
â”‚ â”œâ”€â”€ train/ # 70% of dataset
â”‚ â”‚  â”œâ”€â”€ images/ # jpg files
â”‚ â”‚  â””â”€â”€ labels/ # txt files
â”‚ â””â”€â”€ val/  # 20 % of dataset
â”‚    â”œâ”€â”€ images/ # jpg files
â”‚    â””â”€â”€ labels/ # txt files
â”‚
â”œâ”€â”€ labelled_data/ # raw dataset from kaggle
â”‚ â”œâ”€â”€ images/ # high-res original images
â”‚ â””â”€â”€ labels/ # resp labels of each img from Roboflow
â”‚
â”œâ”€â”€ Models/ # save trained YOLO models from train.py
â”‚
â”œâ”€â”€ tests/ # 10% testing dataset
â”‚    â”œâ”€â”€ input/ # input data for test
â”‚    â””â”€â”€ output/ # output of test
â”‚
â”œâ”€â”€ waldoData/ # 10% testing dataset
â”‚    â”œâ”€â”€ Clean/ # input data for test
â”‚    â”‚ â”œâ”€â”€ ClearedWaldos/ # bg imgs with no waldo
â”‚    â”‚ â””â”€â”€ OnlyWaldoHeads/ # only waldo heads
â”‚    â”œâ”€â”€ NotWaldo/ # imgs populate from generateData.py
â”‚    â””â”€â”€ Waldo/ # imgs populate from generateData.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ customLib.py # python utility functions
â”œâ”€â”€ DataAugmentation # generate training data****
â”œâ”€â”€ inference.py # predict on test data
â”œâ”€â”€ Main # generate training data****
â”œâ”€â”€ Model # generate training data****
â”œâ”€â”€ preprocess.py # preprocess data****
â”œâ”€â”€ README.md 
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train.py # python script to train data
â””â”€â”€ waldo.yaml # yaml file (to be referenced by train.py)
```

---

## ðŸš€ Features

1. Tiling of large illustrations into YOLO-compatible sub-images

2. Annotation handling for images using [RoboFLow](https://app.roboflow.com/)

3. YOLOv11n small-object optimization

4. Steps: preprocessing â†’ training â†’ evaluation â†’ inference

![Project Structure](/assets/images/waldov2.png "Project Structure")

---

### Installation

1. **Clone the repository**

   ```bash
   git clone git@github.com:ACM40960/where-is-waldo.git
   cd where-is-waldo

2. **Create a virtual environment** (optional but recommended)

    ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate

3. **Install dependencies**

    ```bash
    pip install -r requirements.txt

4. **Install PyTorch** (choose version for your CUDA)

   ```bash
    pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8
    
---

### Dataset Preparation

1. Download [Where's Waldo dataset](https://www.kaggle.com/datasets/residentmario/wheres-waldo) from Kaggle

2. Create labels using Roboflow and place original images and labels under labelled_data/images and labelled_data/labels respectively.

3. Run preprocessing to tile large pages into 640Ã—640 chips with YOLO-format labels.

    ```bash
    python preprocess.py

By default, preprocess.py writes to ./datasets/train/ (change dest_path to ./datasets/val/ for validation split).

4. (OPTIONAL) Generate extra Training Data - Use onlyWaldoHeads to paste on clear backgrounds with some rotations into 640Ã—640 chips. Images with Waldo will populate in waldoData/Waldo folder. 

    4.1 Repeat step 2 to create new labels

    4.2 Append datasets/train and datasets/val accordingly.

    ```bash
    python generateData.py

### Model Configuration
![YOLO Model Architecture](/assets/images/YOLOv11.wbep "YOLOv11 Architecture")

The YOLOv11 model is configured for object detection using the Ultralytics CLI with a lightweight pre-trained backbone (yolo11s.pt). It is trained on custom data for 500 epochs with a batch size of 10, using an image resolution of 640Ã—640 pixels. A weight decay of 0.0005 is applied to prevent overfitting. The training is guided by the data.yaml file, which defines the dataset structure, including class names and paths to images. This setup ensures efficient and robust learning tailored for detecting Waldo in the provided dataset.



### Training the Model
The YOLOv11 model was trained for object detection using the lightweight yolo11s.pt backbone. Training was performed for 500 epochs with a batch size of 10 and an image size of 640Ã—640 pixels. A weight decay of 0.0005 was applied to regularize the model and reduce overfitting. The dataset path and class configuration were defined in the data.yaml file, and training output (including weights and logs) was saved automatically by Ultralytics.

```bash
    !yolo task=detect mode=train data={dataset.location}/data.yaml model="yolo11s.pt" epochs=500 batch=10 weight_decay=0.0005 imgsz=640
```


### Validation :
Model performance was evaluated on the validation set using the best model checkpoint saved during training. The validation metrics (such as mAP, precision, and recall) were calculated based on the annotations defined in the dataset.

### Testing:

For final testing and inference, the best trained model was used to make predictions on the test images. A confidence threshold of 0.573 was set to filter detections. The predictions were saved for further visualization and review.

```bash
    !yolo task=detect mode=predict model="/content/runs/detect/train/weights/best.pt" conf=0.573 source={dataset.location}/test/images save=True
```

If the performance metrics are satisfactory, the best-performing model weights are exported and saved for deployment.
If not, the training process is revisited with adjusted hyperparameters for further improvement.