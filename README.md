
# Where's Waldo? â€“ Automated Visual Search using YOLOv11

![Find Waldo Illustration](/assets/images/1.jpg "Find Waldo Illustration")

## ðŸ“œ Overview

This project implements an automated visual search system to detect Waldo in dense illustrations using the YOLOv11 object detection model. Each illustration is split into 640Ã—640 sub-images, with bounding box annotations for Waldo when present. The pipeline is optimized for small-object detection and rare-class scenarios, using tailored hyperparameters and targeted data augmentation.

---

## ðŸ“‚ Project Structure

```bash
where-is-waldo/
â”œâ”€â”€ assets/ # assets for readme
â”‚ â””â”€â”€ images/ # images for README.md
â”‚
â””â”€â”€ data.yaml # yaml file (to be referenced by train.py)
â”œâ”€â”€ datasets/ # train and val datasets (populated from preprocess.py ignored in git due to huge dataset)
â”‚ â”œâ”€â”€ train/ # 70% of dataset
â”‚ â”‚  â”œâ”€â”€ images/ # jpg files
â”‚ â”‚  â””â”€â”€ labels/ # txt files
â”‚ â””â”€â”€ val/  # 20 % of dataset
â”‚    â”œâ”€â”€ images/ # jpg files
â”‚    â””â”€â”€ labels/ # txt files
â”‚
â”œâ”€â”€ labelled_data/ # raw dataset from kaggle ignored in git upload due to huge dataset
â”‚ â”œâ”€â”€ images/ # high-res original images
â”‚ â””â”€â”€ labels/ # resp labels of each img from Roboflow
|
â”‚â”€â”€ models/ #ignored in git upload 
â”‚ â”œâ”€â”€ yolo11n_custom.pt # pretrained weights (model configuration)
|
â”œâ”€â”€ tests/ # 10% testing dataset ignored in git upload due to huge dataset
â”‚    â”œâ”€â”€ input/ # input data for test
â”‚    â””â”€â”€ output/ # output of test
â”‚
â”œâ”€â”€ waldoData/ # 10% testing dataset ignored in git upload due to huge dataset
â”‚    â”œâ”€â”€ Clean/ # input data for test
â”‚    â”‚ â”œâ”€â”€ ClearedWaldos/ # bg imgs with no waldo
â”‚    â”‚ â””â”€â”€ OnlyWaldoHeads/ # only waldo heads
â”‚    â”œâ”€â”€ NotWaldo/ # imgs populate from generateData.py
â”‚    â””â”€â”€ Waldo/ # imgs populate from generateData.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ customLib.py # python utility functions
â”œâ”€â”€ generateData.py # generate training data****
â”œâ”€â”€ inference.py # predict on test data
â”œâ”€â”€ preprocess.py # preprocess data****
â”œâ”€â”€ README.md 
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ train.py # python script to train data
```

---

## ðŸš€ Features

1. Tiling of large illustrations into YOLO-compatible sub-images

2. Annotation handling for images using [RoboFLow](https://app.roboflow.com/)

3. YOLOv11n small-object optimization

4. Steps: preprocessing â†’ training â†’ evaluation â†’ inference

![Project Structure](/assets/images/waldov2.png "Project Structure")

---

### Installation

1. **Clone the repository**

   ```bash
   git clone git@github.com:ACM40960/where-is-waldo.git
   cd where-is-waldo

2. **Create a virtual environment** (optional but recommended)

    ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate

3. **Install dependencies**

    ```bash
    pip install -r requirements.txt

4. **Install PyTorch** (choose version for your CUDA)

   ```bash
    pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118 # Example for CUDA 11.8
    
5. **Define all User Defined Functions** (define and run all functions to be used in below steps)

   ```bash
    python customLib.py
    ```
    
---

### Dataset Preparation

1. Download [Where's Waldo dataset](https://www.kaggle.com/datasets/residentmario/wheres-waldo) from Kaggle

2. Create labels using Roboflow and place original images and labels under labelled_data/images and labelled_data/labels respectively.

3. Run preprocessing to tile large pages into 640Ã—640 chips with YOLO-format labels.

    ```bash
    python preprocess.py

By default, preprocess.py writes to ./datasets/train/ (change dest_path to ./datasets/val/ for validation split).

4. (OPTIONAL) Generate extra Training Data - Use onlyWaldoHeads to paste on clear backgrounds with some rotations into 640Ã—640 chips. Images with Waldo will populate in waldoData/Waldo folder. 

    4.1 Repeat step 2 to create new labels

    4.2 Append datasets/train and datasets/val accordingly.

    ```bash
    python generateData.py

## Model Configuration
![YOLO Model Architecture](/assets/images/YOLOv11.webp "YOLOv11 Architecture")

In our model, we use YOLOv11 (You Only Look Once version 11), a state-of-the-art real-time object detection architecture. YOLOv11 enhances detection accuracy and speed by incorporating lightweight backbone networks, improved anchor-free detection heads, and dynamic label assignment strategies. For our configuration, we fine-tuned YOLOv11 with custom dataset-specific hyperparameters, including image size, batch size, learning rate, and training epochs, ensuring optimal performance for the target detection task.

### Training :
The YOLOv11 model was trained for object detection using the lightweight **yolo11n.pt** backbone. Training was performed for 500 epochs with a batch size of 10 and an image size of 640Ã—640 pixels. A weight decay of 0.0005 was applied to regularize the model and reduce overfitting. The dataset path and class configuration were defined in the **data.yaml** file, and training output (including weights and logs) was saved automatically by Ultralytics.

```bash
    python train.py
```


### Validation :
Model performance was evaluated on the validation set using the best model checkpoint saved during training. The validation metrics (such as mAP, precision, and recall) were calculated based on the annotations defined in the dataset.

```bash
    python validate.py
```
### Testing:

For final testing and inference, the best trained model was used to make predictions on the test images. A confidence threshold of 0.573 was set to filter detections. The predictions were saved for further visualization and review.

```bash
    python inference.py
```

If the performance metrics are satisfactory, the best-performing model weights are exported and saved for deployment.
If not, the training process is revisited with adjusted hyperparameters for further improvement.

### Result : There he is !
The model is now able to detect Waldo accurately within the images, demonstrating the effectiveness of our object detection pipeline.

![Found Waldo](/assets/images/Found_Waldo.png "Found Waldo")